from preprocessing.bert_sentence_embedding import generate_embeddings
from nltk import tokenize


# law_embeddings =
# fin_embeddings =
#
# for law_vec in law_vecs:
#     for fin_vecs in
#
#



